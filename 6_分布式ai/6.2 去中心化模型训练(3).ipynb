{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_hiiw1m3"
   },
   "source": [
    "# 联邦学习扩展"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "graffitiCellId": "id_d36oapi"
   },
   "source": [
    "## <span class=\"motutor-highlight motutor-id_d36oapi-id_zo1e4yx\"><i></i>纵向联邦学习</span>\n",
    "\n",
    "<img src=\"https://imgbed.momodel.cn/VFL1.png\" width=750 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "graffitiCellId": "id_ys1g9w6"
   },
   "source": [
    "## <span class=\"motutor-highlight motutor-id_ys1g9w6-id_ug036ja\"><i></i>去隐私计算</span>\n",
    "\n",
    "<img src=\"https://imgbed.momodel.cn/VFL2.png\" width=750 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "graffitiCellId": "id_estf1nb"
   },
   "source": [
    "<img src=\"https://imgbed.momodel.cn/VFL3.png\" width=750 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "graffitiCellId": "id_03f049g"
   },
   "source": [
    "## <span class=\"motutor-highlight motutor-id_03f049g-id_ssblh05\"><i></i>以线性回归为例</span>",
    "\n",
    "<img src=\"https://imgbed.momodel.cn/VFL4.png\" width=750 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "graffitiCellId": "id_oyi1qfr"
   },
   "source": [
    "<img src=\"https://imgbed.momodel.cn/VFL5.png\" width=750 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "graffitiCellId": "id_bcwnhix"
   },
   "source": [
    "<img src=\"https://imgbed.momodel.cn/VFL6.png\" width=750 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "graffitiCellId": "id_cufuiad"
   },
   "source": [
    "<img src=\"https://imgbed.momodel.cn/VFL7.png\" width=750 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "graffitiCellId": "id_hp85jlm"
   },
   "source": [
    "<img src=\"https://imgbed.momodel.cn/VFL8.png\" width=750 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "graffitiCellId": "id_kjlrubu"
   },
   "source": [
    "## <span class=\"motutor-highlight motutor-id_kjlrubu-id_23tn4mi\"><i></i>Split Learning</span>\n",
    "\n",
    "<img src=\"https://imgbed.momodel.cn/split_new.png\" width=750 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "graffitiCellId": "id_px0jhdx"
   },
   "source": [
    "### <span class=\"motutor-highlight motutor-id_px0jhdx-id_pfst6au\"><i></i>本地异构模型</span>\n",
    "\n",
    "联邦学习中的不同客户端由于特征不同，可以使用不同的模型进行推断。\n",
    "\n",
    "<img src=\"https://imgbed.momodel.cn/input.png\" width=750 />\n",
    "\n",
    "\n",
    "如在输入法预测问题中，不同的客户端可能拥有不同的语言习惯。虽然模型个性化（fine-tune）可以改善本地模型的性能，提高本地预测能力，但是一种更加完善的方法是在训练时考虑更适合本地的异构模型。因此探索如何不同上下文的模型结构搜索仍然是一个重要的开放问题，有可能极大地提高联邦学习训练的模型的实用性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "graffitiCellId": "id_f12gjmx"
   },
   "source": [
    "\n",
    "### 多任务学习\n",
    "\n",
    "联邦学习中不同客户端可能面临不同的预测任务。\n",
    "\n",
    "<img src=\"https://imgbed.momodel.cn/multitask.jpg\" width=750 />\n",
    "\n",
    "在多任务学习中，每个任务都需要单独训练一个模型。如果把联邦学习中每个客户端的学习（本地数据集上的学习问题）都看成是不同的任务，那么在联邦学习中引入多任务学习技术可能会有较大的帮助。\n",
    "\n",
    "多任务学习在跨设备的联邦学习可能较难应用，但适用于较多跨孤岛场景，尤其是多家机构的合作学习。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "graffitiCellId": "id_divg7si"
   },
   "source": [
    "### 本地微调和元学习\n",
    "\n",
    "本地微调是指通过联邦学习训练全局模型，然后将模型部署到所有的客户端后，在使用（推理）前，先用本地的数据集通过额外的训练达到个性化的效果。个性化在非联邦学习中，一般使用微调、迁移学习、域自适应等方法。个性化引入联邦学习的关键在于这些方法在联邦学习的场景下能否保证其学习效果。\n",
    "\n",
    "此外，这些个性化方法通常只假设有一对域（源域和目标域），因此可能会丢失联邦学习的一些较丰富的结构（多个域）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "graffitiCellId": "id_ylnxtkg"
   },
   "source": [
    "\n",
    "另一种研究个性化的方法是元学习。\n",
    "\n",
    "特别是不可知元学习（MAML）算法，采用一种Learn to learn(LTL)的思路，仅使用几次局部梯度迭代就可以训练出一种较优的初始化模型，无需关注模型内部结构。 MAML算法与联邦平均算法密切相关，已经有学者研究了FL和MAML之间的联系，并证明了MAML是一种可以被联邦学习作为初始化训练的框架。\n",
    "\n",
    "将FL和MAML的思想相结合方向比较新，因此存在许多未解决的问题：\n",
    "\n",
    "- 个性化对于FL至关重要。但是，现有的工作没有一个明确的衡量个性化表现的综合指标。例如，对于所有客户端的小改进是否比对一部分客户的大改进要更好？\n",
    "- 具有相同结构和性能，但经过不同训练的模型可以具有非常不同的个性化能力。以最大化全局性能为目标去训模型似乎可能会损害模型的后续个性化能力？\n",
    "- 在此多任务/Learn to learn (LTL)框架中，是否可以引入更多的场景，如：终身学习（lifelong learning）？\n",
    "- 非参数传递的LTL算法（例如原型网络ProtoNets）是否可以用于FL？"
   ]
  }
 ],
 "metadata": {
  "graffiti": {
   "firstAuthorId": "dev",
   "id": "id_md4ioe8",
   "language": "EN"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
